{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ATM808IT/latex-vorlage-fiae/blob/master/Welcome_to_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwFnJsE6vjf8"
      },
      "outputs": [],
      "source": [
        "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"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "id": "P3diM6M66oy4",
        "outputId": "7aa48ea7-aa27-4bbe-ac51-bae097090345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n",
            "Successfully installed groq-0.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq"
      ],
      "metadata": {
        "id": "VcdkCPWJ66DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut 'Command/Ctrl+Enter'. To edit the code, just click the cell and start editing.\n",
        "\n",
        "Variables that you define in one cell can later be used in other cells:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groq_client = Groq(\n",
        "    api_key=(\"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\"),\n",
        ")"
      ],
      "metadata": {
        "id": "L_oIqhYO7CuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_70B = \"llama-3.1-70b-versatile\"\n",
        "llama_405B = \"llama-3.1-405b-reasoning\""
      ],
      "metadata": {
        "id": "Mryl7EkK0cQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = groq_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model= llama_70B,\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "78v6aCWt747p",
        "outputId": "f06ff351-2e07-4726-e401-7ccfcb24f88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in today's technology landscape for several reasons, reflecting their impact on efficiency, user experience, and the broader applications of artificial intelligence (AI) in various sectors. Here are some key points highlighting the importance of fast language models:\n",
            "\n",
            "1. **Real-time Interaction**: Fast language models enable real-time interaction in applications such as chatbots, virtual assistants, and instant messaging platforms. This immediacy is essential for providing users with a seamless and responsive experience, which is critical for maintaining user engagement and satisfaction.\n",
            "\n",
            "2. **Efficiency in Processing Large Volumes of Data**: The ability to process and analyze large volumes of text data quickly is vital for applications in data analytics, research, and business intelligence. Fast language models can handle vast amounts of data more efficiently, leading to quicker insights and decision-making processes.\n",
            "\n",
            "3. **Resource Optimization**: Faster models require less computational power and energy to process the same amount of data compared to slower models. This efficiency is not only cost-effective but also environmentally friendly, as it reduces the carbon footprint associated with data processing.\n",
            "\n",
            "4. **Enhanced Accessibility**: Fast language models can be deployed on a wider range of devices, including those with limited computational resources. This makes advanced language processing capabilities accessible to more people, especially in regions with less robust technological infrastructure.\n",
            "\n",
            "5. **Competitive Advantage**: In the competitive landscape of technology and business, the speed at which companies can process and act on information can be a significant differentiator. Fast language models can provide businesses with a competitive edge by enabling quicker response times, faster data analysis, and more agile decision-making.\n",
            "\n",
            "6. **Improved Accuracy Over Time**: The faster a model can process and learn from data, the quicker it can adapt to changes in language usage, new terminology, and evolving cultural references. This adaptability is crucial for maintaining the accuracy and relevance of language models over time.\n",
            "\n",
            "7. **Enhanced User Experience in Multilingual Support**: For applications that need to support multiple languages, fast language models can quickly adapt to different languages, providing a more inclusive and responsive experience for users worldwide.\n",
            "\n",
            "8. **Critical for Real-time Applications**: In applications such as real-time translation, fast language models are not just beneficial but necessary. The ability to translate text or speech in real-time is crucial for facilitating communication across language barriers in various settings, including international business, travel, and emergency services.\n",
            "\n",
            "9. **Impact on Education and Research**: Fast language models can significantly enhance educational tools and research platforms by providing immediate feedback, suggestions, and analysis. This can improve learning outcomes and accelerate research in fields that heavily rely on text analysis.\n",
            "\n",
            "10. **Future Development and Innovation**: The development of fast language models drives innovation in AI and related technologies. Breakthroughs in language processing speed can lead to advancements in other areas of AI, such as computer vision and speech recognition, and can inspire new applications and services that we cannot yet anticipate.\n",
            "\n",
            "In summary, fast language models are essential for enhancing user experience, improving efficiency, and driving innovation across various sectors. Their importance will only continue to grow as technology advances and the demand for real-time, intelligent, and accessible language processing capabilities increases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q together"
      ],
      "metadata": {
        "id": "nCDB49JY4Aw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from together import Together"
      ],
      "metadata": {
        "id": "HEKWPT5m3YeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "together_client = Together (api_key=\"fd4f9671a51e86f99def825b03d68c8647240b87276ddaf5cc10aa636425c541\")"
      ],
      "metadata": {
        "id": "rBHwM50x4cyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = together_client.chat.completions.create(\n",
        "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"you are a helpful assistant, always answer kindly. use bullet points and examples,\"\n",
        "        },\n",
        "        {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"please teach me neuro science\"\n",
        "        },\n",
        "        {\n",
        "                \"role\": \"assistant\", # Added role to the third message\n",
        "                \"content\": \"\"  # You can leave the content empty if you want the model to generate a response\n",
        "        }\n",
        "],\n",
        "    max_tokens=1013,\n",
        "    temperature=0.11,\n",
        "    top_p=1,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1,\n",
        "    stop=[\"<|eot_id|>\"],\n",
        "    # stream=True\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "tqa6sycd7ePM",
        "outputId": "50e41f09-465e-4bc5-c423-840b55b6daf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What an exciting topic! Neuroscience is a vast and fascinating field that explores the intricacies of the human brain and nervous system. I'd be delighted to help you get started. Here's a comprehensive introduction to neuroscience, broken down into manageable chunks:\n",
            "\n",
            "**What is Neuroscience?**\n",
            "\n",
            "* Neuroscience is the study of the structure and function of the brain and nervous system.\n",
            "* It's an interdisciplinary field that combines biology, psychology, chemistry, physics, and mathematics to understand the complexities of the human brain.\n",
            "* Neuroscience aims to understand how the brain processes information, controls behavior, and responds to various stimuli.\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "* **Neurons**: The building blocks of the brain, responsible for transmitting and processing information.\n",
            "* **Synapses**: The gaps between neurons where chemical signals (neurotransmitters) are exchanged.\n",
            "* **Neurotransmitters**: Chemical messengers that transmit signals between neurons, such as dopamine, serotonin, and acetylcholine.\n",
            "* **Brain regions**: Different areas of the brain, each with unique functions, such as the cerebral cortex, basal ganglia, and hippocampus.\n",
            "\n",
            "**Brain Structure:**\n",
            "\n",
            "* **Cerebrum**: The largest part of the brain, responsible for processing sensory information, controlling movement, and managing higher-level cognitive functions.\n",
            "* **Cerebellum**: Coordinates movement, balance, and posture.\n",
            "* **Brainstem**: Connects the cerebrum to the spinal cord, regulating basic functions like breathing, heart rate, and blood pressure.\n",
            "* **Frontal lobe**: Involved in decision-making, problem-solving, and motor control.\n",
            "* **Parietal lobe**: Processes sensory information related to touch and spatial awareness.\n",
            "* **Temporal lobe**: Plays a key role in auditory processing, memory, and language.\n",
            "\n",
            "**Neurotransmitters and Their Functions:**\n",
            "\n",
            "* **Dopamine**: Involved in reward processing, motivation, and movement control.\n",
            "* **Serotonin**: Regulates mood, appetite, and sleep.\n",
            "* **Acetylcholine**: Plays a role in muscle contraction, memory formation, and attention.\n",
            "* **GABA**: An inhibitory neurotransmitter that helps regulate the activity of other neurons.\n",
            "\n",
            "**Neuroscience Techniques:**\n",
            "\n",
            "* **Functional Magnetic Resonance Imaging (fMRI)**: Measures changes in blood flow to map brain activity.\n",
            "* **Electroencephalography (EEG)**: Records electrical activity in the brain.\n",
            "* **Magnetoencephalography (MEG)**: Measures the magnetic fields generated by electrical activity in the brain.\n",
            "* **Neuropsychological tests**: Assess cognitive functions like attention, memory, and language.\n",
            "\n",
            "**Neurological and Psychiatric Disorders:**\n",
            "\n",
            "* **Alzheimer's disease**: A progressive neurological disorder characterized by memory loss and cognitive decline.\n",
            "* **Parkinson's disease**: A neurodegenerative disorder affecting movement and motor control.\n",
            "* **Depression**: A mood disorder involving altered neurotransmitter levels and brain activity.\n",
            "* **Anxiety disorders**: Conditions like generalized anxiety, panic disorder, and social anxiety disorder.\n",
            "\n",
            "**Current Research and Applications:**\n",
            "\n",
            "* **Neuroplasticity**: The brain's ability to reorganize and adapt in response to experience and learning.\n",
            "* **Neurostimulation**: Techniques like transcranial magnetic stimulation (TMS) and transcranial direct current stimulation (tDCS) to modulate brain activity.\n",
            "* **Neuroprosthetics**: Devices that restore or replace damaged brain function, such as cochlear implants and brain-computer interfaces.\n",
            "\n",
            "This is just a starting point, and there's much more to explore in the world of neuroscience! What specific topics would you like to dive deeper into?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_input():\n",
        "  user_input = input(\"Please enter your question: \")\n",
        "  return user_"
      ],
      "metadata": {
        "id": "Y_JoJuKb_zT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "_77flFGPD22a",
        "outputId": "faada076-e3e3-49c5-f103-78e30b601c52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.37.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.37.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "Successfully installed openai-1.37.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "hI7NNraxD4NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI  # Import the OpenAI"
      ],
      "metadata": {
        "id": "-AyLnDBsEDJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openaiclient = OpenAI(api_key=\"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\", base_url=\"https://api.perplexity.ai\")"
      ],
      "metadata": {
        "id": "Zu_dwWXCCxxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat completion without streaming\n",
        "messages = [  # Define the messages list\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
        "]\n",
        "response = openaiclient.chat.completions.create(\n",
        "    model=\"llama-3-sonar-large-32k-online\",\n",
        "    messages=messages,\n",
        ")\n",
        "print(response)\n",
        "\n",
        "# chat completion with streaming\n",
        "response_stream = openaiclient.chat.completions.create(\n",
        "    model=\"llama-3-sonar-large-32k-online\",\n",
        "    messages=messages,  # Use the same messages list\n",
        "    stream=True,\n",
        ")\n",
        "for response in response_stream:\n",
        "    print(response)\n"
      ],
      "metadata": {
        "id": "QTbKsiOGEQAG",
        "outputId": "5f3c9fd0-8a7e-4e81-bab1-7162807583d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='a1ab5b3f-b542-4832-993f-e28f8567ad5b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here\\'s a joke for you:\\n\\nA penguin is driving across the country when his car starts making a strange noise. He pulls into a garage, and the mechanic says, \"Give me an hour.\" The penguin goes to a diner next door, gets some lunch, and then has some ice cream for dessert. Since penguins don\\'t have hands, he makes a mess scooping up the ice cream with his flippers. When he returns to the garage, the mechanic says, \"Looks like you blew a seal.\" The penguin replies, \"No, no, I just had some ice cream!\"', role='assistant', function_call=None, tool_calls=None), delta={'role': 'assistant', 'content': ''})], created=1722534983, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=126, prompt_tokens=11, total_tokens=137))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content='Here', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': 'Here'})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=11, total_tokens=12))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content=\"'s a\", function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': \"Here's a\"})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=3, prompt_tokens=11, total_tokens=14))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content=' joke for', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': \"Here's a joke for\"})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=11, total_tokens=16))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content=' you:\\n\\n', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': \"Here's a joke for you:\\n\\n\"})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=11, total_tokens=18))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content='What did', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': \"Here's a joke for you:\\n\\nWhat did\"})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=11, total_tokens=20))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content=' one snow', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': \"Here's a joke for you:\\n\\nWhat did one snow\"})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=11, prompt_tokens=11, total_tokens=22))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content='man say', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': \"Here's a joke for you:\\n\\nWhat did one snowman say\"})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=13, prompt_tokens=11, total_tokens=24))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content=' to the', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': \"Here's a joke for you:\\n\\nWhat did one snowman say to the\"})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=11, total_tokens=26))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content=' other?\\n\\n', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': \"Here's a joke for you:\\n\\nWhat did one snowman say to the other?\\n\\n\"})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=11, total_tokens=28))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content='\"Do you', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': 'Here\\'s a joke for you:\\n\\nWhat did one snowman say to the other?\\n\\n\"Do you'})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=19, prompt_tokens=11, total_tokens=30))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content=' smell carrots', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': 'Here\\'s a joke for you:\\n\\nWhat did one snowman say to the other?\\n\\n\"Do you smell carrots'})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=21, prompt_tokens=11, total_tokens=32))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content='?\"', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None, message={'role': 'assistant', 'content': 'Here\\'s a joke for you:\\n\\nWhat did one snowman say to the other?\\n\\n\"Do you smell carrots?\"'})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=25, prompt_tokens=11, total_tokens=36))\n",
            "ChatCompletionChunk(id='dbd5bc95-90e9-4587-91ee-07681e968408', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason='stop', index=0, logprobs=None, message={'role': 'assistant', 'content': 'Here\\'s a joke for you:\\n\\nWhat did one snowman say to the other?\\n\\n\"Do you smell carrots?\"'})], created=1722534984, model='llama-3-sonar-large-32k-online', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=25, prompt_tokens=11, total_tokens=36))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}